{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a21e054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (16, 64, 512)             44032     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (16, 64, 256)             787456    \n",
      "                                                                 \n",
      " dropout (Dropout)           (16, 64, 256)             0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (16, 64, 256)             525312    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (16, 64, 256)             0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (16, 64, 256)             525312    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (16, 64, 256)             0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (16, 64, 86)             22102     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " activation (Activation)     (16, 64, 86)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,904,214\n",
      "Trainable params: 1,904,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding\n",
    "\n",
    "\n",
    "MODEL_DIR = r'D:\\Courses\\0\\1 Programming\\Music-Generation-using-deep-learning-main\\modelx'\n",
    "\n",
    "def save_weights(epoch, model):\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "    model.save_weights(os.path.join(MODEL_DIR, 'weights.{}.h5'.format(epoch)))\n",
    "\n",
    "def load_weights(epoch, model):\n",
    "    model.load_weights(os.path.join(MODEL_DIR, 'weights.{}.h5'.format(epoch)))\n",
    "\n",
    "def build_model(batch_size, seq_len, vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 512, batch_input_shape=(batch_size, seq_len)))\n",
    "    for i in range(3):\n",
    "        model.add(LSTM(256, return_sequences=True, stateful=True))\n",
    "        # It creates 256 lstms layers in hiden layers\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(vocab_size))) \n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = build_model(16, 64, 86)\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d8837ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "NAME = \"Cats-vs-dogs-CNN\"\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "DATA_DIR = r'D:\\Courses\\0\\1 Programming\\Music-Generation-using-deep-learning-main\\data/'\n",
    "LOG_DIR = r'D:\\Courses\\0\\1 Programming\\Music-Generation-using-deep-learning-main\\logs\\log.csv'\n",
    "\n",
    "BATCH_SIZE = 16 #batch_size\n",
    "SEQ_LENGTH = 64 #sequence length\n",
    "\n",
    "def read_batches(T, vocab_size):\n",
    "    length = T.shape[0]; #129,665\n",
    "    batch_chars = int(length / BATCH_SIZE); # 8,104\n",
    "\n",
    "    for start in range(0, batch_chars - SEQ_LENGTH, SEQ_LENGTH): # (0, 8040, 64)\n",
    "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH)) # 16X64\n",
    "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, vocab_size)) # 16X64X86\n",
    "        for batch_idx in range(0, BATCH_SIZE): # (0,16)\n",
    "            for i in range(0, SEQ_LENGTH): #(0,64)\n",
    "                X[batch_idx, i] = T[batch_chars * batch_idx + start + i] # \n",
    "                Y[batch_idx, i, T[batch_chars * batch_idx + start + i + 1]] = 1\n",
    "        yield X, Y\n",
    "\n",
    "def train(text, epochs=100, save_freq=10): \n",
    "    # text will contain input.txt file and i want to save my model at the end of every 10 epochs\n",
    "\n",
    "    # character to index and vice-versa mappings\n",
    "    char_to_idx = { ch: i for (i, ch) in enumerate(sorted(list(set(text)))) }\n",
    "    print(\"Number of unique characters: \" + str(len(char_to_idx))) # unique characters = 86\n",
    "\n",
    "    with open(os.path.join(DATA_DIR, 'char_to_idx.json'), 'w') as f:\n",
    "        json.dump(char_to_idx, f)\n",
    "\n",
    "    idx_to_char = { i: ch for (ch, i) in char_to_idx.items() }\n",
    "    vocab_size = len(char_to_idx)\n",
    "\n",
    "    #model_architecture\n",
    "    model = build_model(BATCH_SIZE, SEQ_LENGTH, vocab_size)\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    #Train data generation\n",
    "    T = np.asarray([char_to_idx[c] for c in text], dtype=np.int32) #convert complete text into numerical indices\n",
    "\n",
    "    print(\"Length of text:\" + str(T.size)) #129,665\n",
    "\n",
    "    steps_per_epoch = (len(text) / BATCH_SIZE - 1) / SEQ_LENGTH  #126 batches per each epoch\n",
    "    \n",
    "    epoch_number, loss_num, acc_num = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n",
    "        epoch_number.append(epoch+1)\n",
    "        losses, accs = [], []\n",
    "\n",
    "        for i, (X, Y) in enumerate(read_batches(T, vocab_size)):\n",
    "            \n",
    "            print(X);\n",
    "\n",
    "            loss, acc = model.train_on_batch(X, Y)\n",
    "#             model.fit(X,Y,BATCH_SIZE, epoch, validation_split=0.3,callbacks=[tensorboard])\n",
    "            print('Batch {}: loss = {}, acc = {}'.format(i + 1, loss, acc))\n",
    "            losses.append(loss)\n",
    "            accs.append(acc)\n",
    "            loss_num.append(loss)\n",
    "        acc_num.append(acc)\n",
    "\n",
    "#         if (epoch + 1) % save_freq == 0:\n",
    "#             save_weights(epoch + 1, model)\n",
    "#             print('Saved checkpoint to', 'weights.{}.h5'.format(epoch + 1))\n",
    "        save_weights(epoch + 1, model)\n",
    "        print('Saved checkpoint to', 'weights.{}.h5'.format(epoch + 1))\n",
    "\n",
    "    #creating dataframe and record all the losses and accuracies at each epoch\n",
    "    log_frame = pd.DataFrame(columns = [\"Epoch\", \"Loss\", \"Accuracy\"])\n",
    "    log_frame[\"Epoch\"] = epoch_number\n",
    "    log_frame[\"Loss\"] = loss_num\n",
    "    log_frame[\"Accuracy\"] = acc_num\n",
    "    log_frame.to_csv(r\"D:\\Courses\\0\\1 Programming\\Music-Generation-using-deep-learning-main\\logs\\log.csv\", index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbda10f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = open(os.path.join(DATA_DIR, 'input.txt'), mode = 'r')\n",
    "data = file.read()\n",
    "file.close()\n",
    "if __name__ == \"__main__\":\n",
    "    train(data, epochs=10, save_freq=10 )\n",
    "#     plt.plot(model.model['val_accuracy'])\n",
    "#     plt.legend(['accuracy','validation accuracy'])\n",
    "# epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaee24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(r\"D:\\Courses\\0\\1 Programming\\Music-Generation-using-deep-learning-main\\logs\\log.csv\")\n",
    "log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a0165c",
   "metadata": {},
   "source": [
    "- Batch Size = 16\n",
    "- Sequence length = 64\n",
    "- Total length of characters in input.txt file = 129,665\n",
    "- No of unique characters = 86\n",
    "- Here, in char_to_idx, char-to-idx is converting every character to a index or numerical value where ch(character) is the key and index or numerical value is the value created in this dictionary.\n",
    "    - which is jusk like json file :\n",
    "    {\"\\n\": 0, \" \": 1, \"!\": 2, \"\\\"\": 3, \"#\": 4, \"%\": 5, \"&\": 6, \"'\": 7, \"(\": 8, \")\": 9, \"+\": 10, \",\": 11, \"-\": 12, \".\": 13, \"/\": 14, \"0\": 15, \"1\": 16, \"2\": 17, \"3\": 18, \"4\": 19, \"5\": 20, \"6\": 21, \"7\": 22, \"8\": 23, \"9\": 24, \":\": 25, \"=\": 26, \"?\": 27, \"A\": 28, \"B\": 29, \"C\": 30, \"D\": 31, \"E\": 32, \"F\": 33, \"G\": 34, \"H\": 35, \"I\": 36, \"J\": 37, \"K\": 38, \"L\": 39, \"M\": 40, \"N\": 41, \"O\": 42, \"P\": 43, \"Q\": 44, \"R\": 45, \"S\": 46, \"T\": 47, \"U\": 48, \"V\": 49, \"W\": 50, \"X\": 51, \"Y\": 52, \"[\": 53, \"\\\\\": 54, \"]\": 55, \"^\": 56, \"_\": 57, \"a\": 58, \"b\": 59, \"c\": 60, \"d\": 61, \"e\": 62, \"f\": 63, \"g\": 64, \"h\": 65, \"i\": 66, \"j\": 67, \"k\": 68, \"l\": 69, \"m\": 70, \"n\": 71, \"o\": 72, \"p\": 73, \"q\": 74, \"r\": 75, \"s\": 76, \"t\": 77, \"u\": 78, \"v\": 79, \"w\": 80, \"x\": 81, \"y\": 82, \"z\": 83, \"|\": 84, \"~\": 85}\n",
    "    - Our indices starts from 0 to 85 as the number of unique characters in vocabulary is 86.\n",
    "- I'm trying to generate new batch everytime using the function caleed \"read_batches\".\n",
    "- X is a matrix of (BATCH_SIZE,SEQ_LENGTH) = (16,64)\n",
    "- Y is a 3D tensor of (BATCH_SIZE,SEQ_LENGTH,vocab_size) = (16,64,86). The vocab size is considered because of one-hot encoding.\n",
    "- After embedding, (BACTH_SIZE,SEQ_LENGTH,embedding_dim) = (16,64,512)\n",
    "- We encoded \"Y\" as one hot encoded because we will be applying softmax on top of it.\n",
    "- Now, we want to predict the next character which should be one of the 86 unique characters. So, it's a multi-class classification problem. Therefore, our last layer is softmax layer of 86 activations.\n",
    "- So, I will generating each of my batches and train them. For every training epoch, I will print the categorical crossentropy loss and accuracy.\n",
    "- Inthe summary of our model, we can observe that, we have 1,904,214 total parameters.\n",
    "- As we are having so many parameters, so we are using dropouts with keep probability of 0.2.\n",
    "- By the time we reach 100 epochs while training, roughly around 90% + times, the model is able to predict what the next character is. So, our model is doing a pretty good job.\n",
    "- At the end of 10 epochs, we are storing the weights of the model. We will use these weights to reconstruct the model and predict."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
